{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Lab Solutions - REINFORCE for CartPole\n",
    "\n",
    "\n",
    "This notebook contains complete solutions with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1 Solution: Policy Network\n",
    "\n",
    "**Solution:**\n",
    "- Hidden dimension: 32 neurons (a good default for small problems)\n",
    "- Activation: ReLU (most common choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for CartPole.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # SOLUTION: Set hidden dimension to 32\n",
    "        hidden_dim = 32\n",
    "        \n",
    "        # SOLUTION: Use ReLU activation\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),  # SOLUTION\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state ‚Üí action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "# Test the network\n",
    "policy = PolicyNetwork()\n",
    "test_state = torch.FloatTensor([0.0, 0.0, 0.1, 0.0])\n",
    "logits = policy(test_state)\n",
    "\n",
    "print(f\"Input state shape: {test_state.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Output logits: {logits}\")\n",
    "print(\"\\n‚úì Network architecture correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2 Solution: Action Selection\n",
    "\n",
    "**Key concepts:**\n",
    "1. Use `softmax` to convert logits ‚Üí probabilities\n",
    "2. Use `torch.multinomial` to sample from probability distribution\n",
    "3. Return both action and log probability (needed for gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for CartPole.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=2):\n",
    "        super().__init__()\n",
    "        hidden_dim = 32\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state ‚Üí action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample an action from the policy.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        \n",
    "        # SOLUTION: Convert logits to probabilities using softmax\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # SOLUTION: Sample action from categorical distribution\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        \n",
    "        # Compute log probability\n",
    "        log_prob = torch.log(probs[action])\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# Test action selection\n",
    "policy = PolicyNetwork()\n",
    "test_state = np.array([0.0, 0.0, 0.1, 0.0])\n",
    "\n",
    "print(\"Testing action selection (5 trials):\")\n",
    "for i in range(5):\n",
    "    action, log_prob = policy.select_action(test_state)\n",
    "    prob = torch.exp(log_prob).item()\n",
    "    print(f\"  Trial {i+1}: action={action}, log_prob={log_prob.item():.3f}, prob={prob:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Action selection working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3 Solution: Computing Discounted Returns\n",
    "\n",
    "**The core of REINFORCE!**\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "R = 0\n",
    "For each reward r (from end to start):\n",
    "    R = r + Œ≥ * R\n",
    "    Insert R at beginning of returns\n",
    "```\n",
    "\n",
    "**Why backwards?** Because G_t depends on G_{t+1}, so we need to compute from the end.\n",
    "\n",
    "**Example:**\n",
    "- Rewards: [1, 1, 1, 1, 1]\n",
    "- Œ≥ = 0.9\n",
    "- Returns:\n",
    "  - G_4 = 1\n",
    "  - G_3 = 1 + 0.9 * 1 = 1.9\n",
    "  - G_2 = 1 + 0.9 * 1.9 = 2.71\n",
    "  - G_1 = 1 + 0.9 * 2.71 = 3.439\n",
    "  - G_0 = 1 + 0.9 * 3.439 = 4.0951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns for each timestep.\n",
    "    \n",
    "    G_t = r_t + Œ≥*r_{t+1} + Œ≥¬≤*r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    R = 0  # Running return\n",
    "    \n",
    "    # SOLUTION: Iterate backwards through rewards\n",
    "    for r in reversed(rewards):\n",
    "        # SOLUTION: Bellman equation for return\n",
    "        R = r + gamma * R\n",
    "        # SOLUTION: Insert at beginning to maintain order\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    return torch.FloatTensor(returns)\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "test_rewards = [1, 1, 1, 1, 1]\n",
    "test_gamma = 0.9\n",
    "\n",
    "returns = compute_returns(test_rewards, test_gamma)\n",
    "\n",
    "print(f\"Rewards: {test_rewards}\")\n",
    "print(f\"Gamma: {test_gamma}\")\n",
    "print(f\"\\nComputed returns: {returns}\")\n",
    "\n",
    "# Manual calculation for verification\n",
    "print(\"\\nManual verification:\")\n",
    "print(f\"  G_4 = 1 = {1}\")\n",
    "print(f\"  G_3 = 1 + 0.9*1 = {1 + 0.9*1}\")\n",
    "print(f\"  G_2 = 1 + 0.9*1.9 = {1 + 0.9*1.9}\")\n",
    "print(f\"  G_1 = 1 + 0.9*2.71 = {1 + 0.9*2.71}\")\n",
    "print(f\"  G_0 = 1 + 0.9*3.439 = {1 + 0.9*3.439}\")\n",
    "\n",
    "expected = torch.FloatTensor([4.0951, 3.439, 2.71, 1.9, 1.0])\n",
    "if torch.allclose(returns, expected, atol=0.01):\n",
    "    print(\"\\n‚úì Correct! Returns match expected values.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Expected: {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4 Solution: Policy Gradient Loss\n",
    "\n",
    "**The REINFORCE loss:**\n",
    "$$L = -\\sum_{t=0}^T \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "**Intuition:**\n",
    "- High return G_t ‚Üí increase log probability ‚Üí increase P(action)\n",
    "- Low return G_t ‚Üí decrease log probability ‚Üí decrease P(action)\n",
    "- Negative sign: minimize loss = maximize return\n",
    "\n",
    "**Two implementation approaches:**\n",
    "1. Loop version (explicit)\n",
    "2. Vectorized version (efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_loss(log_probs, returns):\n",
    "    \"\"\"\n",
    "    Compute REINFORCE policy gradient loss.\n",
    "    \"\"\"\n",
    "    # SOLUTION (Method 1): Loop version\n",
    "    loss = 0\n",
    "    for log_prob, G in zip(log_probs, returns):\n",
    "        loss += -log_prob * G\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_policy_loss_vectorized(log_probs, returns):\n",
    "    \"\"\"\n",
    "    SOLUTION (Method 2): Vectorized version - more efficient.\n",
    "    \"\"\"\n",
    "    # Stack log probs into a single tensor\n",
    "    log_probs_tensor = torch.stack(log_probs)\n",
    "    # Element-wise multiply and sum\n",
    "    loss = -(log_probs_tensor * returns).sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test both implementations\n",
    "test_log_probs = [\n",
    "    torch.tensor(-0.5),\n",
    "    torch.tensor(-0.7),\n",
    "    torch.tensor(-0.6)\n",
    "]\n",
    "test_returns = torch.FloatTensor([3.0, 2.0, 1.0])\n",
    "\n",
    "loss1 = compute_policy_loss(test_log_probs, test_returns)\n",
    "loss2 = compute_policy_loss_vectorized(test_log_probs, test_returns)\n",
    "\n",
    "print(f\"Log probs: {[lp.item() for lp in test_log_probs]}\")\n",
    "print(f\"Returns: {test_returns.tolist()}\")\n",
    "print(f\"\\nLoop version loss: {loss1.item():.3f}\")\n",
    "print(f\"Vectorized loss: {loss2.item():.3f}\")\n",
    "\n",
    "# Manual calculation:\n",
    "# L = -(-0.5*3.0 + -0.7*2.0 + -0.6*1.0) = -(-1.5 - 1.4 - 0.6) = 3.5\n",
    "expected_loss = 3.5\n",
    "print(f\"\\nExpected loss: {expected_loss}\")\n",
    "\n",
    "if abs(loss1.item() - expected_loss) < 0.01:\n",
    "    print(\"\\n‚úì Correct! Both methods produce the expected loss.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Check calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Training Loop\n",
    "\n",
    "Now we put everything together and train the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, policy, optimizer, n_episodes=1000, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train policy using REINFORCE algorithm.\n",
    "    Uses all the functions we implemented above.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Sample an episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob = policy.select_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # Normalize returns (variance reduction)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_policy_loss(log_probs, returns)\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track performance\n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Create environment and policy\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training REINFORCE on CartPole-v1...\")\n",
    "print(\"This will take a few minutes.\\n\")\n",
    "\n",
    "rewards = train_reinforce(env, policy, optimizer, n_episodes=1000)\n",
    "\n",
    "final_avg = np.mean(rewards[-100:])\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Final 100-episode average: {final_avg:.2f}\")\n",
    "\n",
    "if final_avg >= 475:\n",
    "    print(f\"\\nüéâ SUCCESS! Agent solved CartPole!\")\n",
    "else:\n",
    "    print(f\"\\nAgent is learning but needs more training.\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Raw rewards + Moving average\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards, alpha=0.3, color='blue', label='Episode reward')\n",
    "\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, \n",
    "             color='red', linewidth=2, label=f'{window}-episode MA')\n",
    "\n",
    "plt.axhline(y=475, color='g', linestyle='--', alpha=0.7, label='Success threshold')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Reward distribution\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "early_rewards = rewards[:min(200, len(rewards)//2)]\n",
    "late_rewards = rewards[max(len(rewards)//2, len(rewards)-200):]\n",
    "\n",
    "plt.hist(early_rewards, bins=20, alpha=0.5, label='Early episodes', color='blue')\n",
    "plt.hist(late_rewards, bins=20, alpha=0.5, label='Late episodes', color='red')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Rolling statistics\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "window = 100\n",
    "if len(rewards) >= window:\n",
    "    rolling_mean = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    rolling_std = [np.std(rewards[max(0, i-window):i]) for i in range(window, len(rewards)+1)]\n",
    "    \n",
    "    x = range(window-1, len(rewards))\n",
    "    plt.plot(x, rolling_mean, label='Mean', color='blue', linewidth=2)\n",
    "    plt.fill_between(x, \n",
    "                     rolling_mean - np.array(rolling_std), \n",
    "                     rolling_mean + np.array(rolling_std),\n",
    "                     alpha=0.3, color='blue', label='¬±1 std')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title(f'{window}-Episode Rolling Statistics')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(f\"  First 100 episodes avg: {np.mean(rewards[:100]):.2f}\")\n",
    "print(f\"  Last 100 episodes avg:  {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"  Best episode:           {max(rewards):.0f}\")\n",
    "print(f\"  Worst episode:          {min(rewards):.0f}\")\n",
    "print(f\"  Overall average:        {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_episode(env, policy, max_steps=500):\n",
    "    \"\"\"Record and display an episode as animation.\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Render\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Select action (greedy - no exploration)\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            logits = policy(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            action = torch.argmax(probs).item()\n",
    "        \n",
    "        # Step\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Episode lasted {step + 1} steps, total reward: {total_reward:.0f}\")\n",
    "    \n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Trained Policy (Reward: {total_reward:.0f})', fontsize=14)\n",
    "    img = ax.imshow(frames[0])\n",
    "    \n",
    "    def animate(i):\n",
    "        img.set_data(frames[i])\n",
    "        return [img]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), \n",
    "                                   interval=20, blit=True, repeat=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "# Display trained policy\n",
    "print(\"Watching trained agent...\\n\")\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "display_episode(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluating the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    \"\"\"Evaluate trained policy over multiple episodes.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Greedy action selection\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            with torch.no_grad():\n",
    "                logits = policy(state_tensor)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.argmax(probs).item()\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating agent over 100 episodes...\\n\")\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_rewards = evaluate_policy(env, policy, n_episodes=100)\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"  Mean reward:   {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"  Std deviation: {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Min reward:    {np.min(eval_rewards):.0f}\")\n",
    "print(f\"  Max reward:    {np.max(eval_rewards):.0f}\")\n",
    "print(f\"  Median reward: {np.median(eval_rewards):.0f}\")\n",
    "\n",
    "if np.mean(eval_rewards) >= 475:\n",
    "    print(f\"\\n‚úì Agent successfully solved CartPole!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Agent needs more training to consistently solve CartPole.\")\n",
    "\n",
    "# Plot evaluation distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(eval_rewards, bins=20, edgecolor='black')\n",
    "plt.axvline(np.mean(eval_rewards), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(eval_rewards):.1f}')\n",
    "plt.axvline(475, color='g', linestyle='--', linewidth=2, label='Solved: 475')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Evaluation Reward Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_rewards, marker='o', linestyle='-', alpha=0.6)\n",
    "plt.axhline(np.mean(eval_rewards), color='r', linestyle='--', label=f'Mean: {np.mean(eval_rewards):.1f}')\n",
    "plt.axhline(475, color='g', linestyle='--', label='Solved: 475')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Evaluation Episodes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5 Solution (Optional): Value Baseline\n",
    "\n",
    "This is a preview of **Actor-Critic** methods (next week)!\n",
    "\n",
    "**Idea:** Use a value network V(s) to estimate expected return, then compute **advantages**:\n",
    "$$A_t = G_t - V(s_t)$$\n",
    "\n",
    "The advantage tells us: \"How much better was this action than average?\"\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces variance ‚Üí more stable learning\n",
    "- Faster convergence\n",
    "- Better final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for estimating V(s).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # SOLUTION: Network outputs a single value (not a vector!)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Single output\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Estimate value of state.\"\"\"\n",
    "        return self.network(state).squeeze()\n",
    "\n",
    "\n",
    "def train_with_value_baseline(env, policy, value_net, policy_opt, value_opt, \n",
    "                               n_episodes=1000, gamma=0.99):\n",
    "    \"\"\"Train with learned value baseline (Actor-Critic style).\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        states = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            \n",
    "            # SOLUTION: Get value estimate\n",
    "            value = value_net(state_tensor)\n",
    "            \n",
    "            # Get action\n",
    "            action, log_prob = policy.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # SOLUTION: Compute advantages\n",
    "        values_tensor = torch.stack(values)\n",
    "        advantages = returns - values_tensor.detach()  # detach to not backprop through value\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # SOLUTION: Update policy using advantages\n",
    "        policy_loss = compute_policy_loss(log_probs, advantages)\n",
    "        \n",
    "        policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_opt.step()\n",
    "        \n",
    "        # SOLUTION: Update value network to predict returns better\n",
    "        value_loss = ((values_tensor - returns) ** 2).mean()\n",
    "        \n",
    "        value_opt.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_opt.step()\n",
    "        \n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Train with value baseline\n",
    "print(\"Training with value baseline (Actor-Critic style)...\\n\")\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy_baseline = PolicyNetwork()\n",
    "value_net = ValueNetwork()\n",
    "policy_opt = optim.Adam(policy_baseline.parameters(), lr=0.01)\n",
    "value_opt = optim.Adam(value_net.parameters(), lr=0.01)\n",
    "\n",
    "rewards_baseline = train_with_value_baseline(\n",
    "    env, policy_baseline, value_net, policy_opt, value_opt, \n",
    "    n_episodes=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal avg with baseline: {np.mean(rewards_baseline[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare REINFORCE vs Actor-Critic style\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "window = 50\n",
    "\n",
    "# REINFORCE\n",
    "if len(rewards) >= window:\n",
    "    ma_reinforce = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), ma_reinforce, \n",
    "             label='REINFORCE', linewidth=2)\n",
    "\n",
    "# With value baseline\n",
    "if len(rewards_baseline) >= window:\n",
    "    ma_baseline = np.convolve(rewards_baseline, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards_baseline)), ma_baseline, \n",
    "             label='With Value Baseline (Actor-Critic)', linewidth=2)\n",
    "\n",
    "plt.axhline(y=475, color='g', linestyle='--', alpha=0.7, label='Solved')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(f'Reward ({window}-episode MA)')\n",
    "plt.title('Comparison: REINFORCE vs Actor-Critic Style')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  REINFORCE final avg:           {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"  Actor-Critic style final avg:  {np.mean(rewards_baseline[-100:]):.2f}\")\n",
    "print(\"\\nThe value baseline typically provides:\")\n",
    "print(\"  - Faster initial learning\")\n",
    "print(\"  - More stable training\")\n",
    "print(\"  - Lower variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary of Solutions\n",
    "\n",
    "### Task 1: Policy Network\n",
    "```python\n",
    "hidden_dim = 32\n",
    "activation = nn.ReLU()\n",
    "```\n",
    "\n",
    "### Task 2: Action Selection\n",
    "```python\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "action = torch.multinomial(probs, 1).item()\n",
    "```\n",
    "\n",
    "### Task 3: Compute Returns\n",
    "```python\n",
    "for r in reversed(rewards):\n",
    "    R = r + gamma * R\n",
    "    returns.insert(0, R)\n",
    "```\n",
    "\n",
    "### Task 4: Policy Loss\n",
    "```python\n",
    "loss = 0\n",
    "for log_prob, G in zip(log_probs, returns):\n",
    "    loss += -log_prob * G\n",
    "```\n",
    "\n",
    "### Task 5 (Optional): Value Baseline\n",
    "```python\n",
    "# Value network\n",
    "nn.Linear(state_dim, hidden_dim) ‚Üí nn.ReLU() ‚Üí nn.Linear(hidden_dim, 1)\n",
    "\n",
    "# Advantages\n",
    "advantages = returns - values_tensor.detach()\n",
    "\n",
    "# Value loss\n",
    "value_loss = ((values_tensor - returns) ** 2).mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully implemented REINFORCE from scratch and trained an agent to solve CartPole!\n",
    "\n",
    "### What You Learned\n",
    "1. **Policy networks** map states to action probabilities\n",
    "2. **Stochastic sampling** enables exploration\n",
    "3. **Discounted returns** assign credit to actions\n",
    "4. **Policy gradients** directly optimize policy parameters\n",
    "5. **Baselines** reduce variance and improve learning\n",
    "\n",
    "### Next Week: Actor-Critic\n",
    "We'll combine policy gradients with value functions for even better performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
