{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wTJnMM5Q0tl"
      },
      "source": [
        "# Lab: Deep RL with LunarLander\n",
        "\n",
        "In this lab, you'll use **stable-baselines3**, a popular RL library, to train agents on the LunarLander environment. You'll experiment with different algorithms and hyperparameters to understand what works and why.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ The LunarLander Task\n",
        "\n",
        "**Goal:** Land a spacecraft safely on the landing pad\n",
        "\n",
        "**State (8 values):**\n",
        "- x, y position\n",
        "- x, y velocity\n",
        "- angle, angular velocity\n",
        "- left leg contact, right leg contact\n",
        "\n",
        "**Actions (4 discrete):**\n",
        "- 0: Do nothing\n",
        "- 1: Fire left engine\n",
        "- 2: Fire main engine\n",
        "- 3: Fire right engine\n",
        "\n",
        "**Rewards:**\n",
        "- +100 to +140 for landing safely\n",
        "- -100 for crashing\n",
        "- Small penalties for firing engines (fuel efficiency)\n",
        "- Bonus for legs touching ground\n",
        "\n",
        "**Success:** Average reward ‚â• 200 over 100 episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qezrF14cQ0tq"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DqFEqGIQ0tq",
        "outputId": "459e60ca-ed32-4ecb-febc-29164e971de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (4,401 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y swig\n",
        "!pip install stable-baselines3[extra] gymnasium[box2d] -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra] gymnasium[box2d] -q"
      ],
      "metadata": {
        "id": "72unaXwSVTXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mLGMO4GpQ0ts"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import PPO, A2C, DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qlZHoLAQ0ts"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 1: Exploring the Environment\n",
        "\n",
        "Let's first understand the environment by watching a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WefvrCcJQ0ts",
        "outputId": "31a58960-2d14-4eaa-ae1c-7622ff48bbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment Info:\n",
            "  Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "  Action space: Discrete(4)\n",
            "  Actions: 0=nothing, 1=left, 2=main, 3=right\n",
            "\n",
            "Initial state: [ 0.00229702  1.4181306   0.2326471   0.3204666  -0.00265488 -0.05269805\n",
            "  0.          0.        ]\n",
            "  [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Create environment\n",
        "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
        "\n",
        "print(\"Environment Info:\")\n",
        "print(f\"  Observation space: {env.observation_space}\")\n",
        "print(f\"  Action space: {env.action_space}\")\n",
        "print(f\"  Actions: 0=nothing, 1=left, 2=main, 3=right\")\n",
        "\n",
        "# Test random policy\n",
        "state, _ = env.reset(seed=42)\n",
        "print(f\"\\nInitial state: {state}\")\n",
        "print(f\"  [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9wan0ZWQ0tt",
        "outputId": "7e3b1b2c-0ca9-49d2-9736-39578c46301a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating random policy...\n",
            "\n",
            "Random Policy Performance:\n",
            "  Mean reward: -231.46\n",
            "  Std: 131.79\n",
            "  Min: -471.82\n",
            "  Max: -66.91\n",
            "\n",
            "Random agents typically crash! (rewards around -200)\n"
          ]
        }
      ],
      "source": [
        "def evaluate_random_policy(env, n_episodes=10):\n",
        "    \"\"\"Evaluate a random policy.\"\"\"\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample()  # Random action\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            #print(state, reward, terminated, truncated)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "    return episode_rewards\n",
        "\n",
        "\n",
        "# Evaluate random policy\n",
        "print(\"Evaluating random policy...\\n\")\n",
        "random_rewards = evaluate_random_policy(env, n_episodes=10)\n",
        "\n",
        "print(f\"Random Policy Performance:\")\n",
        "print(f\"  Mean reward: {np.mean(random_rewards):.2f}\")\n",
        "print(f\"  Std: {np.std(random_rewards):.2f}\")\n",
        "print(f\"  Min: {np.min(random_rewards):.2f}\")\n",
        "print(f\"  Max: {np.max(random_rewards):.2f}\")\n",
        "print(f\"\\nRandom agents typically crash! (rewards around -200)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2LRFTSFQ0tu"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 2: Training Your First Agent (PPO)\n",
        "\n",
        "**PPO (Proximal Policy Optimization)** is a popular, reliable algorithm that works well out-of-the-box.\n",
        "\n",
        "### What you'll do:\n",
        "1. Create a PPO agent with default settings\n",
        "2. Train for a fixed number of timesteps\n",
        "3. Evaluate the trained agent\n",
        "\n",
        "**Key hyperparameters to notice:**\n",
        "- `learning_rate`: How fast the agent learns\n",
        "- `n_steps`: How many steps to collect before updating\n",
        "- `batch_size`: Size of minibatches for optimization\n",
        "- `n_epochs`: Number of optimization epochs per update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcvaLi1cQ0tu",
        "outputId": "c6c55b9b-3df7-4b9a-b1e0-969e8cb86f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "\n",
            "Created PPO agent with default hyperparameters\n",
            "Ready to train!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Create environment\n",
        "env = gym.make('LunarLander-v3')\n",
        "\n",
        "# Create PPO agent with DEFAULT settings\n",
        "model_ppo = PPO(\n",
        "    'MlpPolicy',           # Multi-layer perceptron policy\n",
        "    env,\n",
        "    learning_rate=3e-4,    # Default learning rate\n",
        "    n_steps=2048,          # Number of steps to collect per update\n",
        "    batch_size=64,         # Minibatch size\n",
        "    n_epochs=10,           # Number of epochs per update\n",
        "    gamma=0.99,            # Discount factor\n",
        "    verbose=1,             # Print training progress\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\nCreated PPO agent with default hyperparameters\")\n",
        "print(\"Ready to train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlkvyIEHQ0tv",
        "outputId": "1144197a-8859-4f65-f0ba-17c851316018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PPO agent...\n",
            "This will take ~5 minutes\n",
            "\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 86.7     |\n",
            "|    ep_rew_mean     | -165     |\n",
            "| time/              |          |\n",
            "|    fps             | 1094     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 91.3        |\n",
            "|    ep_rew_mean          | -155        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 758         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007234049 |\n",
            "|    clip_fraction        | 0.00601     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.000897    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 658         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.004      |\n",
            "|    value_loss           | 1.29e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 92.6         |\n",
            "|    ep_rew_mean          | -154         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 720          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0095348265 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.0122      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 687          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00877     |\n",
            "|    value_loss           | 1.14e+03     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 98.4        |\n",
            "|    ep_rew_mean          | -171        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 717         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011809772 |\n",
            "|    clip_fraction        | 0.0769      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | -0.000701   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 448         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    value_loss           | 864         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 103         |\n",
            "|    ep_rew_mean          | -168        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 708         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011709782 |\n",
            "|    clip_fraction        | 0.107       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.000166    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 324         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    value_loss           | 1.03e+03    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 108         |\n",
            "|    ep_rew_mean          | -164        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 689         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009306602 |\n",
            "|    clip_fraction        | 0.067       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | -0.0198     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 330         |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0058     |\n",
            "|    value_loss           | 673         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 114         |\n",
            "|    ep_rew_mean          | -165        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 684         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 20          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008205125 |\n",
            "|    clip_fraction        | 0.0897      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | -0.00809    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 194         |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00712    |\n",
            "|    value_loss           | 506         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | -174         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 684          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 23           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0144738145 |\n",
            "|    clip_fraction        | 0.0619       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | -0.000326    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 424          |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00686     |\n",
            "|    value_loss           | 723          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | -176         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 685          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068942904 |\n",
            "|    clip_fraction        | 0.029        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | -0.00357     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 391          |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00349     |\n",
            "|    value_loss           | 987          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | -161        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 669         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 30          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008023776 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | -6.68e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 313         |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00493    |\n",
            "|    value_loss           | 459         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 136         |\n",
            "|    ep_rew_mean          | -147        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 668         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010345988 |\n",
            "|    clip_fraction        | 0.0912      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | -2.56e-05   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 151         |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00676    |\n",
            "|    value_loss           | 279         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | -149        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 669         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 36          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010409482 |\n",
            "|    clip_fraction        | 0.0896      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 8e-05       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 157         |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00928    |\n",
            "|    value_loss           | 412         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 151         |\n",
            "|    ep_rew_mean          | -153        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 670         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006079656 |\n",
            "|    clip_fraction        | 0.0738      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.000523    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 359         |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    value_loss           | 753         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 156          |\n",
            "|    ep_rew_mean          | -138         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 662          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045532025 |\n",
            "|    clip_fraction        | 0.0202       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.0894       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 98.1         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00619     |\n",
            "|    value_loss           | 480          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 168          |\n",
            "|    ep_rew_mean          | -129         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 663          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012150335 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.17        |\n",
            "|    explained_variance   | 0.181        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 271          |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    value_loss           | 398          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 178          |\n",
            "|    ep_rew_mean          | -116         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 662          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 49           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018018584 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.249        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 187          |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00223     |\n",
            "|    value_loss           | 246          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 183          |\n",
            "|    ep_rew_mean          | -103         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 663          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 52           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030869185 |\n",
            "|    clip_fraction        | 0.00713      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.493        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 42.1         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00264     |\n",
            "|    value_loss           | 175          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 188          |\n",
            "|    ep_rew_mean          | -102         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 657          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 56           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038419312 |\n",
            "|    clip_fraction        | 0.00127      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.12        |\n",
            "|    explained_variance   | 0.424        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 186          |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.00313     |\n",
            "|    value_loss           | 392          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 194          |\n",
            "|    ep_rew_mean          | -99.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 658          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 59           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012740967 |\n",
            "|    clip_fraction        | 0.000635     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.12        |\n",
            "|    explained_variance   | 0.238        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 420          |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00256     |\n",
            "|    value_loss           | 423          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 208          |\n",
            "|    ep_rew_mean          | -101         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 659          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 62           |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0095492015 |\n",
            "|    clip_fraction        | 0.0499       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0.415        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 110          |\n",
            "|    n_updates            | 190          |\n",
            "|    policy_gradient_loss | -0.00953     |\n",
            "|    value_loss           | 275          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 218        |\n",
            "|    ep_rew_mean          | -95.3      |\n",
            "| time/                   |            |\n",
            "|    fps                  | 660        |\n",
            "|    iterations           | 21         |\n",
            "|    time_elapsed         | 65         |\n",
            "|    total_timesteps      | 43008      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00898316 |\n",
            "|    clip_fraction        | 0.0715     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.21      |\n",
            "|    explained_variance   | -0.247     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 192        |\n",
            "|    n_updates            | 200        |\n",
            "|    policy_gradient_loss | -0.0083    |\n",
            "|    value_loss           | 338        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 233         |\n",
            "|    ep_rew_mean          | -87.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 655         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 68          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009379317 |\n",
            "|    clip_fraction        | 0.0527      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.489       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 54.8        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00484    |\n",
            "|    value_loss           | 107         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | -86.1        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 656          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 71           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071575847 |\n",
            "|    clip_fraction        | 0.0517       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.19        |\n",
            "|    explained_variance   | 0.553        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.4         |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00654     |\n",
            "|    value_loss           | 107          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 260         |\n",
            "|    ep_rew_mean          | -85         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 658         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 74          |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012598278 |\n",
            "|    clip_fraction        | 0.103       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.525       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 36.2        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00767    |\n",
            "|    value_loss           | 119         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 270         |\n",
            "|    ep_rew_mean          | -78.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 659         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009317177 |\n",
            "|    clip_fraction        | 0.061       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.352       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 49.8        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00737    |\n",
            "|    value_loss           | 191         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 290         |\n",
            "|    ep_rew_mean          | -73         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 655         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008674309 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.587       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 45.1        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00769    |\n",
            "|    value_loss           | 87.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 298         |\n",
            "|    ep_rew_mean          | -69.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 656         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 84          |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005905663 |\n",
            "|    clip_fraction        | 0.0821      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.52        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 50.6        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00601    |\n",
            "|    value_loss           | 82.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 308         |\n",
            "|    ep_rew_mean          | -66.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 657         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005766514 |\n",
            "|    clip_fraction        | 0.03        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.515       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.3        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0044     |\n",
            "|    value_loss           | 83.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 335         |\n",
            "|    ep_rew_mean          | -62.8       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 658         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 90          |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023522735 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.549       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 37.1        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 347         |\n",
            "|    ep_rew_mean          | -53.1       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 654         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 93          |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010681656 |\n",
            "|    clip_fraction        | 0.0655      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.03        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.00359    |\n",
            "|    value_loss           | 30.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 357          |\n",
            "|    ep_rew_mean          | -51.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 655          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 96           |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058824862 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.03        |\n",
            "|    explained_variance   | 0.563        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.9         |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00633     |\n",
            "|    value_loss           | 134          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 383         |\n",
            "|    ep_rew_mean          | -47         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 656         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008120497 |\n",
            "|    clip_fraction        | 0.0438      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.803       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 52.2        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00802    |\n",
            "|    value_loss           | 46.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 396         |\n",
            "|    ep_rew_mean          | -43.4       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 657         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 102         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010826299 |\n",
            "|    clip_fraction        | 0.0662      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.892       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.4         |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.00616    |\n",
            "|    value_loss           | 16.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 407         |\n",
            "|    ep_rew_mean          | -38.9       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 654         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 106         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007940756 |\n",
            "|    clip_fraction        | 0.0681      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.947      |\n",
            "|    explained_variance   | 0.665       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 78.8        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00704    |\n",
            "|    value_loss           | 115         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 417          |\n",
            "|    ep_rew_mean          | -35.8        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 655          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 109          |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038837092 |\n",
            "|    clip_fraction        | 0.0218       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.95        |\n",
            "|    explained_variance   | 0.707        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 51.3         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    value_loss           | 87.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 435         |\n",
            "|    ep_rew_mean          | -33.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 657         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007802406 |\n",
            "|    clip_fraction        | 0.038       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.983      |\n",
            "|    explained_variance   | 0.768       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.58        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.00445    |\n",
            "|    value_loss           | 53          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 445         |\n",
            "|    ep_rew_mean          | -33.5       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 658         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 115         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008229576 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.908      |\n",
            "|    explained_variance   | 0.789       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 20          |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.00567    |\n",
            "|    value_loss           | 36.3        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 457       |\n",
            "|    ep_rew_mean          | -34.3     |\n",
            "| time/                   |           |\n",
            "|    fps                  | 655       |\n",
            "|    iterations           | 38        |\n",
            "|    time_elapsed         | 118       |\n",
            "|    total_timesteps      | 77824     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0074875 |\n",
            "|    clip_fraction        | 0.0594    |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -0.943    |\n",
            "|    explained_variance   | 0.662     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 16.5      |\n",
            "|    n_updates            | 370       |\n",
            "|    policy_gradient_loss | -0.00834  |\n",
            "|    value_loss           | 50        |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 471          |\n",
            "|    ep_rew_mean          | -33.5        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 656          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047339275 |\n",
            "|    clip_fraction        | 0.0571       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.974       |\n",
            "|    explained_variance   | 0.654        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.6         |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.0069      |\n",
            "|    value_loss           | 62.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 490          |\n",
            "|    ep_rew_mean          | -31          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 657          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061952136 |\n",
            "|    clip_fraction        | 0.0577       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.983       |\n",
            "|    explained_variance   | 0.629        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.9         |\n",
            "|    n_updates            | 390          |\n",
            "|    policy_gradient_loss | -0.00662     |\n",
            "|    value_loss           | 73.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 507         |\n",
            "|    ep_rew_mean          | -25         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 658         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 127         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019143485 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.837      |\n",
            "|    explained_variance   | 0.572       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.78        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.00731    |\n",
            "|    value_loss           | 83.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 523         |\n",
            "|    ep_rew_mean          | -14.7       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 655         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 131         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015868578 |\n",
            "|    clip_fraction        | 0.0392      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.792      |\n",
            "|    explained_variance   | 0.328       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 53.8        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00565    |\n",
            "|    value_loss           | 159         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 536          |\n",
            "|    ep_rew_mean          | -1.71        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 656          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 134          |\n",
            "|    total_timesteps      | 88064        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050191837 |\n",
            "|    clip_fraction        | 0.0603       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.92        |\n",
            "|    explained_variance   | 0.168        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 48.6         |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | -0.00472     |\n",
            "|    value_loss           | 185          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 547         |\n",
            "|    ep_rew_mean          | 7.47        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 658         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008350499 |\n",
            "|    clip_fraction        | 0.0489      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.886      |\n",
            "|    explained_variance   | 0.422       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 94.9        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00555    |\n",
            "|    value_loss           | 165         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 560         |\n",
            "|    ep_rew_mean          | 20          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 659         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 139         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008071897 |\n",
            "|    clip_fraction        | 0.0628      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.941      |\n",
            "|    explained_variance   | 0.655       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 24.2        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    value_loss           | 86.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 572          |\n",
            "|    ep_rew_mean          | 27.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 657          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 143          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044996254 |\n",
            "|    clip_fraction        | 0.033        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.846       |\n",
            "|    explained_variance   | 0.61         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 47.6         |\n",
            "|    n_updates            | 450          |\n",
            "|    policy_gradient_loss | -0.0032      |\n",
            "|    value_loss           | 136          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 587        |\n",
            "|    ep_rew_mean          | 38.4       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 658        |\n",
            "|    iterations           | 47         |\n",
            "|    time_elapsed         | 146        |\n",
            "|    total_timesteps      | 96256      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00813593 |\n",
            "|    clip_fraction        | 0.0446     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.877     |\n",
            "|    explained_variance   | 0.716      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 34.7       |\n",
            "|    n_updates            | 460        |\n",
            "|    policy_gradient_loss | -0.00264   |\n",
            "|    value_loss           | 74.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 597         |\n",
            "|    ep_rew_mean          | 45.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 659         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 149         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004192734 |\n",
            "|    clip_fraction        | 0.0259      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.814      |\n",
            "|    explained_variance   | 0.716       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 30.1        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.0038     |\n",
            "|    value_loss           | 87.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 607         |\n",
            "|    ep_rew_mean          | 53.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 660         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 151         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003591011 |\n",
            "|    clip_fraction        | 0.033       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.926      |\n",
            "|    explained_variance   | 0.378       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 22          |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.00247    |\n",
            "|    value_loss           | 165         |\n",
            "-----------------------------------------\n",
            "\n",
            "Training completed in 153.2 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "print(\"Training PPO agent...\")\n",
        "print(\"This will take ~5 minutes\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "model_ppo.learn(total_timesteps=100_000)  # 100k steps\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nTraining completed in {train_time:.1f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBO10V0YQ0tv",
        "outputId": "e054ad16-a3e7-40a9-cc8d-62c9b26845b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Evaluate trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model_ppo, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"\\nPPO Agent Performance (100 episodes):\")\n",
        "print(f\"  Mean reward: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
        "\n",
        "if mean_reward >= 200:\n",
        "    print(f\"  ‚úì SUCCESS! Agent solved LunarLander!\")\n",
        "else:\n",
        "    print(f\"  Agent is learning but needs more training\")\n",
        "    print(f\"  (Success threshold: 200)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpwxjxCMQ0tv"
      },
      "outputs": [],
      "source": [
        "# Visualize Trained Agent\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def record_video(env_name, model, video_length=500, prefix=''):\n",
        "    \"\"\"\n",
        "    Record a video of the agent playing.\n",
        "    \"\"\"\n",
        "    # Create environment with rgb_array rendering\n",
        "    eval_env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "    obs, _ = eval_env.reset()\n",
        "    frames = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(video_length):\n",
        "        frames.append(eval_env.render())\n",
        "\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "    # Save video\n",
        "    video_path = f\"{prefix}lunar_lander.mp4\"\n",
        "    imageio.mimsave(video_path, frames, fps=30)\n",
        "\n",
        "    return video_path, total_reward\n",
        "\n",
        "\n",
        "# Record and display video\n",
        "print(\"Recording agent performance...\")\n",
        "video_path, episode_reward = record_video('LunarLander-v3', model_ppo, video_length=1000)\n",
        "\n",
        "print(f\"Episode reward: {episode_reward:.2f}\")\n",
        "print(f\"Result: {'‚úì LANDED!' if episode_reward > 200 else '‚úó Crashed'}\")\n",
        "\n",
        "# Display video in Colab\n",
        "from IPython.display import Video\n",
        "Video(video_path, embed=True, width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36kHEPvQ0tw"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 3: Comparing Algorithms\n",
        "\n",
        "Now let's compare PPO with other algorithms:\n",
        "- **A2C (Advantage Actor-Critic)**: Simpler, faster, but less stable\n",
        "- **DQN (Deep Q-Network)**: Value-based, good for discrete actions\n",
        "\n",
        "**Your task:** Train A2C and DQN with similar compute budgets and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siDjxQUkQ0tw"
      },
      "outputs": [],
      "source": [
        "# Train A2C\n",
        "print(\"Training A2C agent...\\n\")\n",
        "\n",
        "env = gym.make('LunarLander-v3')\n",
        "model_a2c = A2C(\n",
        "    'MlpPolicy',\n",
        "    env,\n",
        "    learning_rate=7e-4,    # A2C often needs higher LR\n",
        "    n_steps=5,             # A2C uses fewer steps per update\n",
        "    gamma=0.99,\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "model_a2c.learn(total_timesteps=100_000)\n",
        "mean_a2c, std_a2c = evaluate_policy(model_a2c, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"\\nA2C Performance: {mean_a2c:.2f} ¬± {std_a2c:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGU3ilbZQ0tw"
      },
      "outputs": [],
      "source": [
        "# Train DQN\n",
        "print(\"Training DQN agent...\\n\")\n",
        "\n",
        "env = gym.make('LunarLander-v3')\n",
        "model_dqn = DQN(\n",
        "    'MlpPolicy',\n",
        "    env,\n",
        "    learning_rate=1e-4,\n",
        "    buffer_size=50000,     # Replay buffer size\n",
        "    learning_starts=1000,  # Start learning after this many steps\n",
        "    batch_size=64,\n",
        "    gamma=0.99,\n",
        "    verbose=1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "model_dqn.learn(total_timesteps=100_000)\n",
        "mean_dqn, std_dqn = evaluate_policy(model_dqn, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"\\nDQN Performance: {mean_dqn:.2f} ¬± {std_dqn:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRIs-XlhQ0tx"
      },
      "outputs": [],
      "source": [
        "# Compare all algorithms\n",
        "algorithms = ['Random', 'PPO', 'A2C', 'DQN']\n",
        "mean_rewards = [\n",
        "    np.mean(random_rewards),\n",
        "    mean_reward,\n",
        "    mean_a2c,\n",
        "    mean_dqn\n",
        "]\n",
        "std_rewards = [\n",
        "    np.std(random_rewards),\n",
        "    std_reward,\n",
        "    std_a2c,\n",
        "    std_dqn\n",
        "]\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['red', 'blue', 'green', 'orange']\n",
        "bars = plt.bar(algorithms, mean_rewards, yerr=std_rewards,\n",
        "               capsize=5, color=colors, alpha=0.7)\n",
        "plt.axhline(y=200, color='black', linestyle='--', label='Success threshold (200)')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Algorithm Comparison on LunarLander-v2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, mean in zip(bars, mean_rewards):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{mean:.1f}',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(f\"  - PPO typically performs best (most reliable)\")\n",
        "print(f\"  - A2C learns faster but can be less stable\")\n",
        "print(f\"  - DQN is sample-efficient but sensitive to hyperparameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFwKNibsQ0tx"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 4: Hyperparameter Experiments\n",
        "\n",
        "Now experiment with hyperparameters to see their effect!\n",
        "\n",
        "**Your task:** Try different learning rates with PPO and observe the impact.\n",
        "\n",
        "**What to adjust:**\n",
        "- `learning_rate`: Higher = faster learning but less stable\n",
        "- Try: `1e-4` (low), `3e-4` (default), `1e-3` (high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLa5lYcXQ0tx"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(learning_rate, timesteps=50_000):\n",
        "    \"\"\"Train PPO with given learning rate and evaluate.\"\"\"\n",
        "    env = gym.make('LunarLander-v3')\n",
        "\n",
        "    model = PPO(\n",
        "        'MlpPolicy',\n",
        "        env,\n",
        "        learning_rate=learning_rate,\n",
        "        verbose=0,  # Suppress output for cleaner comparison\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50)\n",
        "\n",
        "    return mean_reward, std_reward\n",
        "\n",
        "\n",
        "# Experiment with different learning rates\n",
        "learning_rates = [1e-4, 3e-4, 1e-3]\n",
        "results = {}\n",
        "\n",
        "print(\"Experimenting with learning rates...\")\n",
        "print(\"(Training 3 agents with 50k steps each - takes ~5 min)\\n\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with LR={lr}...\")\n",
        "    mean, std = train_and_evaluate(lr, timesteps=50_000)\n",
        "    results[lr] = (mean, std)\n",
        "    print(f\"  Result: {mean:.2f} ¬± {std:.2f}\\n\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "lrs = [f\"{lr:.0e}\" for lr in learning_rates]\n",
        "means = [results[lr][0] for lr in learning_rates]\n",
        "stds = [results[lr][1] for lr in learning_rates]\n",
        "\n",
        "plt.bar(lrs, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
        "plt.axhline(y=200, color='g', linestyle='--', label='Success threshold')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Impact of Learning Rate on PPO Performance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nWhat did you observe?\")\n",
        "print(\"  - Too low: Slow learning, might not solve in time\")\n",
        "print(\"  - Default: Good balance\")\n",
        "print(\"  - Too high: Might be unstable or overshoot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILh3ik_cQ0ty"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 5 (Optional): Training Longer\n",
        "\n",
        "The previous experiments used 50k-100k steps for speed. But can we do better with more training?\n",
        "\n",
        "**Your task:** Train PPO for longer (300k steps) and track learning progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPA2GYAkQ0ty"
      },
      "outputs": [],
      "source": [
        "# Create evaluation callback to track progress\n",
        "eval_env = gym.make('LunarLander-v3')\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path='./logs/',\n",
        "    log_path='./logs/',\n",
        "    eval_freq=5000,  # Evaluate every 5k steps\n",
        "    n_eval_episodes=20,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Train for longer\n",
        "env = gym.make('LunarLander-v3')\n",
        "model_long = PPO('MlpPolicy', env, verbose=1, seed=42)\n",
        "\n",
        "print(\"Training PPO for 300k steps...\")\n",
        "print(\"(This takes ~15 minutes)\\n\")\n",
        "\n",
        "model_long.learn(total_timesteps=300_000, callback=eval_callback)\n",
        "\n",
        "# Final evaluation\n",
        "mean_long, std_long = evaluate_policy(model_long, env, n_eval_episodes=100)\n",
        "print(f\"\\nFinal performance: {mean_long:.2f} ¬± {std_long:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYYA-47JQ0ty"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "1. **Using RL libraries**: stable-baselines3 makes it easy to try different algorithms\n",
        "2. **Algorithm comparison**: PPO is reliable, A2C is fast, DQN needs tuning\n",
        "3. **Hyperparameter impact**: Learning rate significantly affects performance\n",
        "4. **Training time**: More training usually helps, but with diminishing returns\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "**What makes RL challenging:**\n",
        "- Sparse rewards (only get signal at landing/crash)\n",
        "- Delayed consequences (actions affect future states)\n",
        "- High variance in training\n",
        "- Sensitive to hyperparameters\n",
        "\n",
        "**What helps:**\n",
        "- Good algorithm choice (PPO is often best default)\n",
        "- Sufficient training time\n",
        "- Appropriate learning rate\n",
        "- Multiple random seeds for robustness\n",
        "\n",
        "### Things You Can Try\n",
        "\n",
        "1. **Different environments**: Try `LunarLanderContinuous-v3` (continuous actions)\n",
        "2. **Network architecture**: Modify `policy_kwargs` to use bigger networks\n",
        "3. **Reward shaping**: Modify the environment to give better feedback\n",
        "4. **Curriculum learning**: Start with easier tasks, gradually increase difficulty\n",
        "5. **Ensemble methods**: Train multiple agents and combine their predictions\n",
        "\n",
        "---\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "You're now ready to work on your project! Choose from:\n",
        "- Atari games (Pong, Breakout)\n",
        "- Autonomous driving (Highway-Env)\n",
        "- Stock trading (Financial RL)\n",
        "- Multi-agent scenarios (PettingZoo)\n",
        "\n",
        "Or propose your own project idea!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}