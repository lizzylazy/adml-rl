{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Atari Pong Baseline\n",
    "\n",
    "This notebook provides a working baseline for training an RL agent on Atari Pong.\n",
    "\n",
    "**Runtime:** ~5 minutes for baseline (100k steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install stable-baselines3[extra] gymnasium[atari,accept-rom-license] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Preprocessed Environment\n",
    "\n",
    "Atari preprocessing:\n",
    "- Convert to grayscale\n",
    "- Resize to 84x84\n",
    "- Stack 4 frames\n",
    "- Frame skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized environment with preprocessing\n",
    "env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=42)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "print(f\"Environment created:\")\n",
    "print(f\"  Observation shape: {env.observation_space.shape}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Number of parallel envs: {env.num_envs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO agent with CNN policy\n",
    "model = PPO(\n",
    "    'CnnPolicy',           # CNN for image input\n",
    "    env,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,           # Steps per update per env\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.1,\n",
    "    ent_coef=0.01,         # Entropy bonus for exploration\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nAgent created. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent\n",
    "print(\"Training for 100k steps (~5 minutes)...\\n\")\n",
    "model.learn(total_timesteps=100_000)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "\n",
    "print(f\"\\nEvaluation Results (20 episodes):\")\n",
    "print(f\"  Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "# In Pong, rewards range from -21 (lose) to +21 (win)\n",
    "# Positive means agent is winning!\n",
    "if mean_reward > 0:\n",
    "    win_rate = (mean_reward + 21) / 42 * 100\n",
    "    print(f\"  Estimated win rate: {win_rate:.1f}%\")\n",
    "    print(f\"  ✓ Agent is winning!\")\n",
    "else:\n",
    "    print(f\"  Agent needs more training\")\n",
    "    print(f\"  Try: model.learn(total_timesteps=500_000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch Agent Play\n",
    "\n",
    "**Note:** Video rendering might not work in all environments. If it fails, the agent is still trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in single environment\n",
    "test_env = gym.make('PongNoFrameskip-v4', render_mode='rgb_array')\n",
    "test_env = gym.wrappers.AtariPreprocessing(test_env, frame_skip=4)\n",
    "test_env = gym.wrappers.FrameStack(test_env, num_stack=4)\n",
    "\n",
    "obs, _ = test_env.reset(seed=42)\n",
    "frames = []\n",
    "episode_reward = 0\n",
    "\n",
    "for _ in range(5000):  # Max steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Record frames (optional)\n",
    "    if len(frames) < 500:  # Save first 500 frames\n",
    "        frames.append(test_env.render())\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Episode reward: {episode_reward}\")\n",
    "print(f\"Result: {'WIN' if episode_reward > 0 else 'LOSE'}\")\n",
    "\n",
    "# Show sample frame\n",
    "if frames:\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    plt.imshow(frames[0])\n",
    "    plt.title(f\"Pong Gameplay (Episode reward: {episode_reward})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working baseline, try:\n",
    "\n",
    "1. **Train longer:** Change `total_timesteps` to 500k or 1M\n",
    "2. **Tune hyperparameters:** Try different learning rates, batch sizes\n",
    "3. **Compare algorithms:** Try DQN instead of PPO\n",
    "4. **Visualize learning:** Track win rate over time\n",
    "\n",
    "See `project1_atari_README.md` for detailed improvement ideas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
