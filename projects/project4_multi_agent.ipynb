{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Multi-Agent RL Baseline\n",
    "\n",
    "Train multiple agents to cooperate in the Simple Spread environment.\n",
    "\n",
    "**Runtime:** ~5 minutes for baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 pettingzoo[mpe] supersuit -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_spread_v3\n",
    "import supersuit as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi-Agent Environment\n",
    "\n",
    "**Simple Spread:**\n",
    "- 3 agents (cooperative)\n",
    "- 3 landmarks\n",
    "- Goal: Each agent covers a different landmark\n",
    "- Reward: Negative distance to nearest uncovered landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base environment\n",
    "env = simple_spread_v3.parallel_env(\n",
    "    N=3,                    # Number of agents\n",
    "    max_cycles=25,          # Episode length\n",
    "    continuous_actions=False\n",
    ")\n",
    "\n",
    "print(f\"Environment: Simple Spread\")\n",
    "print(f\"  Number of agents: {env.num_agents}\")\n",
    "print(f\"  Agents: {env.agents}\")\n",
    "\n",
    "# Reset and show observation space\n",
    "observations, infos = env.reset(seed=42)\n",
    "agent = env.agents[0]\n",
    "\n",
    "print(f\"\\nAgent '{agent}':\")\n",
    "print(f\"  Observation shape: {observations[agent].shape}\")\n",
    "print(f\"  Action space: {env.action_space(agent)}\")\n",
    "print(f\"    (5 actions: no-op, left, right, up, down)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Environment\n",
    "\n",
    "Use SuperSuit to convert parallel environment to single-agent format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap environment for SB3\n",
    "env = simple_spread_v3.parallel_env(N=3, max_cycles=25, continuous_actions=False)\n",
    "\n",
    "# Convert to single-agent format (all agents controlled together)\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, num_vec_envs=1, num_cpus=1, base_class='stable_baselines3')\n",
    "\n",
    "print(\"Environment wrapped for SB3\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agents (Independent PPO)\n",
    "\n",
    "Each agent learns its own policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO agent\n",
    "model = PPO(\n",
    "    MlpPolicy,\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining agents (~5 minutes)...\\n\")\n",
    "model.learn(total_timesteps=100_000)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "def evaluate_multi_agent(model, n_episodes=20):\n",
    "    \"\"\"Evaluate multi-agent policy.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "rewards = evaluate_multi_agent(model, n_episodes=20)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Evaluation (20 episodes):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Mean reward: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "print(f\"  Min: {np.min(rewards):.2f}\")\n",
    "print(f\"  Max: {np.max(rewards):.2f}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Higher (less negative) is better\n",
    "if np.mean(rewards) > -30:\n",
    "    print(f\"\\n✓ Good coordination! Agents spread out well.\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Agents need more training to coordinate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record episode\n",
    "raw_env = simple_spread_v3.parallel_env(\n",
    "    N=3, max_cycles=25, \n",
    "    continuous_actions=False,\n",
    "    render_mode='rgb_array'\n",
    ")\n",
    "\n",
    "observations, infos = raw_env.reset(seed=42)\n",
    "frames = []\n",
    "episode_reward = {agent: 0 for agent in raw_env.agents}\n",
    "\n",
    "for step in range(25):  # Max steps\n",
    "    # Get frame\n",
    "    frames.append(raw_env.render())\n",
    "    \n",
    "    # Simple random policy for visualization\n",
    "    actions = {agent: raw_env.action_space(agent).sample() for agent in raw_env.agents}\n",
    "    \n",
    "    observations, rewards, terminations, truncations, infos = raw_env.step(actions)\n",
    "    \n",
    "    for agent in raw_env.agents:\n",
    "        episode_reward[agent] += rewards[agent]\n",
    "    \n",
    "    if all(terminations.values()) or all(truncations.values()):\n",
    "        break\n",
    "\n",
    "print(f\"Episode rewards per agent:\")\n",
    "for agent, reward in episode_reward.items():\n",
    "    print(f\"  {agent}: {reward:.2f}\")\n",
    "\n",
    "# Show sample frames\n",
    "if len(frames) >= 3:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    indices = [0, len(frames)//2, -1]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(frames[idx])\n",
    "        axes[i].set_title(f\"Step {idx if idx >= 0 else len(frames)+idx}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Multi-Agent Behavior (Blue=Agents, Black=Landmarks)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nNote: Circles are agents, colored squares are landmarks\")\n",
    "print(\"Goal: Each agent should cover a different landmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Coordination\n",
    "\n",
    "Measure how well agents spread out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distance between agents\n",
    "def analyze_coordination(n_episodes=10):\n",
    "    \"\"\"Measure agent spread.\"\"\"\n",
    "    env_test = simple_spread_v3.parallel_env(N=3, max_cycles=25, continuous_actions=False)\n",
    "    \n",
    "    min_distances = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        observations, _ = env_test.reset()\n",
    "        \n",
    "        for step in range(25):\n",
    "            actions = {agent: env_test.action_space(agent).sample() for agent in env_test.agents}\n",
    "            observations, rewards, terminations, truncations, infos = env_test.step(actions)\n",
    "            \n",
    "            # Calculate min distance between agents\n",
    "            # (In real implementation, would extract positions from observations)\n",
    "            \n",
    "            if all(terminations.values()) or all(truncations.values()):\n",
    "                break\n",
    "    \n",
    "    return min_distances\n",
    "\n",
    "print(\"Coordination analysis:\")\n",
    "print(\"  (Higher rewards = better spread)\")\n",
    "print(\"  (Goal: Each agent near different landmark)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Improve multi-agent coordination:\n",
    "\n",
    "1. **Parameter sharing:** Train single policy for all agents\n",
    "2. **Communication:** Add message passing between agents\n",
    "3. **Curriculum:** Start with 2 agents, increase to 3\n",
    "4. **Competitive:** Try Simple Tag (predator-prey)\n",
    "5. **Different algorithms:** MADDPG, QMIX, CommNet\n",
    "\n",
    "See `project4_multi_agent_README.md` for detailed ideas!\n",
    "\n",
    "**Note:** Multi-agent RL is an active research area with many open challenges!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
