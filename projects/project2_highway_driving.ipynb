{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Highway Driving Baseline\n",
    "\n",
    "Train an agent to drive safely on a highway.\n",
    "\n",
    "**Runtime:** ~3 minutes for baseline (50k steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 highway-env pygame -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "The highway environment features:\n",
    "- 5 lanes\n",
    "- Dynamic traffic\n",
    "- 5 discrete actions: [idle, left, right, faster, slower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('highway-fast-v0', render_mode='rgb_array')\n",
    "\n",
    "print(f\"Environment: {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"  (5 vehicles x 5 features: presence, x, y, vx, vy)\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"  0=idle, 1=left, 2=right, 3=faster, 4=slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment (Optional)\n",
    "\n",
    "Customize scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: customize configuration\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 5,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n",
    "        \"absolute\": False,\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 50,\n",
    "    \"duration\": 40,  # seconds\n",
    "    \"reward_speed_range\": [20, 30],\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 1,\n",
    "}\n",
    "\n",
    "env.unwrapped.config.update(config)\n",
    "env.reset()\n",
    "\n",
    "print(\"\\nEnvironment configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show initial state\n",
    "obs, _ = env.reset()\n",
    "img = env.render()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Highway Environment (Bird's Eye View)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nObservation shape: {obs.shape}\")\n",
    "print(f\"First vehicle (ego): {obs[0]}\")\n",
    "print(f\"  [presence, rel_x, rel_y, rel_vx, rel_vy]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN agent\n",
    "model = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=5e-4,\n",
    "    buffer_size=15000,\n",
    "    learning_starts=200,\n",
    "    batch_size=32,\n",
    "    gamma=0.8,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    target_update_interval=50,\n",
    "    exploration_fraction=0.7,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nDQN agent created. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train agent\n",
    "print(\"Training for 50k steps (~3 minutes)...\\n\")\n",
    "model.learn(total_timesteps=50_000)\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation\n",
    "def detailed_evaluate(model, env, n_episodes=20):\n",
    "    \"\"\"Evaluate with detailed metrics.\"\"\"\n",
    "    rewards = []\n",
    "    successes = []\n",
    "    speeds = []\n",
    "    crashes = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_speeds = []\n",
    "        crashed = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Track speed\n",
    "            if 'speed' in info:\n",
    "                episode_speeds.append(info['speed'])\n",
    "            \n",
    "            # Check if crashed\n",
    "            if info.get('crashed', False):\n",
    "                crashed = True\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        successes.append(not crashed)\n",
    "        crashes.append(crashed)\n",
    "        if episode_speeds:\n",
    "            speeds.append(np.mean(episode_speeds))\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'success_rate': np.mean(successes) * 100,\n",
    "        'crash_rate': np.mean(crashes) * 100,\n",
    "        'avg_speed': np.mean(speeds) if speeds else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = detailed_evaluate(model, env, n_episodes=20)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Evaluation Results (20 episodes):\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Mean reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "print(f\"  Success rate: {results['success_rate']:.1f}%\")\n",
    "print(f\"  Crash rate: {results['crash_rate']:.1f}%\")\n",
    "print(f\"  Average speed: {results['avg_speed']:.1f} km/h\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if results['success_rate'] >= 70:\n",
    "    print(f\"\\n✓ Good performance! Agent drives safely.\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Agent needs more training.\")\n",
    "    print(f\"  Try: model.learn(total_timesteps=150_000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record an episode\n",
    "obs, _ = env.reset()\n",
    "frames = []\n",
    "done = False\n",
    "episode_reward = 0\n",
    "\n",
    "while not done and len(frames) < 200:  # Max 200 frames\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    episode_reward += reward\n",
    "    \n",
    "    frames.append(env.render())\n",
    "\n",
    "print(f\"Episode reward: {episode_reward:.2f}\")\n",
    "print(f\"Crashed: {info.get('crashed', False)}\")\n",
    "\n",
    "# Show sample frames\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, idx in enumerate([0, len(frames)//2, -1]):\n",
    "    axes[i].imshow(frames[idx])\n",
    "    axes[i].set_title(f\"Frame {idx if idx >= 0 else len(frames)+idx}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Agent Driving (Reward: {episode_reward:.1f})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have a working baseline:\n",
    "\n",
    "1. **Train longer:** Try 150k steps for better performance\n",
    "2. **Reward shaping:** Modify rewards for safety/speed trade-off\n",
    "3. **Traffic scenarios:** Increase vehicle density\n",
    "4. **Continuous control:** Switch to PPO with continuous actions\n",
    "\n",
    "See `project2_highway_README.md` for detailed ideas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
